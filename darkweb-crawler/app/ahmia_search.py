#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ahmia Search and Bulk Analysis
Combines Ahmia search with TorCrawl analysis for bulk onion site processing
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from .analysis import OnionScrapAnalyzer


class AhmiaSearchAnalyzer:
    def __init__(self, api_key: Optional[str] = None):
        self.base_url = "https://ahmia.fi"
        self.session = requests.Session()
        
        # Configure session with better timeout and retry settings
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # Configure session with better connection settings
        adapter = requests.adapters.HTTPAdapter(
            max_retries=3,
            pool_connections=10,
            pool_maxsize=10
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        self.analyzer = OnionScrapAnalyzer(api_key)
    
    def search_ahmia(self, query: str, max_results: int = 10, days: Optional[int] = None) -> List[Dict]:
        """
        Search Ahmia for the given query using regular internet access
        
        Args:
            query (str): Search term
            max_results (int): Maximum number of results to return
            days (int): Number of days to search back (1, 7, 30)
            
        Returns:
            list: List of search results
        """
        # Build search URL with time parameter if specified
        if days and days in [1, 7, 30]:
            search_url = f"{self.base_url}/search/?q={query}&d={days}"
        else:
            search_url = f"{self.base_url}/search/?q={query}"
        
        print(f"üîç Searching Ahmia via internet: {search_url}")
        
        # Retry logic with exponential backoff
        max_retries = 3
        base_timeout = 15
        
        for attempt in range(max_retries):
            try:
                timeout = base_timeout * (2 ** attempt)  # 15s, 30s, 60s
                print(f"Attempt {attempt + 1}/{max_retries} with {timeout}s timeout...")
                
                # Regular internet request (no Tor proxy)
                response = self.session.get(search_url, timeout=timeout)
                response.raise_for_status()
                
                # Parse the HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Find all result items
                results = []
                result_items = soup.find_all('li', class_='result')
                
                print(f"Found {len(result_items)} raw results, processing...")
                
                for i, item in enumerate(result_items[:max_results]):
                    try:
                        result = self._extract_result(item)
                        if result:
                            results.append(result)
                    except Exception as e:
                        print(f"Error processing result {i+1}: {e}")
                        continue
                
                print(f"Successfully processed {len(results)} results via internet")
                return results
                
            except requests.exceptions.Timeout as e:
                print(f"Timeout on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                else:
                    print("All retry attempts failed due to timeout")
                    return []
                    
            except requests.RequestException as e:
                print(f"Request error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                else:
                    print("All retry attempts failed due to request errors")
                    return []
                    
            except Exception as e:
                print(f"Unexpected error on attempt {attempt + 1}: {e}")
                return []
        
        return []
    
    def _extract_result(self, item) -> Optional[Dict]:
        """Extract data from a single result item"""
        try:
            # Extract name and link
            name_elem = item.find('h4').find('a')
            name = name_elem.get_text(strip=True) if name_elem else "No name"
            
            # Extract onion URL
            onion_url = None
            cite_elem = item.find('cite')
            if cite_elem:
                onion_url = cite_elem.get_text(strip=True)
            
            # Extract description
            description = "No description provided"
            desc_elem = item.find('p')
            if desc_elem:
                desc_text = desc_elem.get_text(strip=True)
                if desc_text and desc_text != "No description provided":
                    description = desc_text
            
            # Extract last seen timestamp
            last_seen = "Unknown"
            last_seen_elem = item.find('span', class_='lastSeen')
            if last_seen_elem:
                last_seen = last_seen_elem.get_text(strip=True)
            
            return {
                'name': name,
                'onion_url': onion_url,
                'description': description,
                'last_seen': last_seen
            }
            
        except Exception as e:
            print(f"Error extracting result: {e}")
            return None
    
    def analyze_onion_site(self, onion_url: str, depth: int = 1, custom_prompt: Optional[str] = None, model: Optional[str] = None, use_langchain: bool = False) -> Dict:
        """
        Analyze a single onion site using TorCrawl and AI analysis
        
        Args:
            onion_url (str): The onion URL to analyze
            depth (int): Crawl depth for TorCrawl
            custom_prompt (str): Custom analysis prompt
            
        Returns:
            dict: Analysis results
        """
        try:
            print(f"Analyzing: {onion_url}")
            result = self.analyzer.run_full_analysis(
                url=onion_url,
                depth=depth,
                custom_prompt=custom_prompt,
                model=model,
                use_langchain=use_langchain,
            )
            return {
                'onion_url': onion_url,
                'analysis_result': result,
                'success': result.get('success', False)
            }
        except Exception as e:
            return {
                'onion_url': onion_url,
                'analysis_result': None,
                'success': False,
                'error': str(e)
            }
    
    def bulk_search_and_analyze(
        self, 
        query: str, 
        max_sites: int = 5, 
        depth: int = 1, 
        custom_prompt: Optional[str] = None,
        model: Optional[str] = None,
        use_langchain: bool = False,
        days: Optional[int] = None
    ) -> Dict:
        """
        Search Ahmia for onion sites and analyze each one
        
        Args:
            query (str): Search term for Ahmia
            max_sites (int): Maximum number of sites to analyze
            depth (int): Crawl depth for TorCrawl
            custom_prompt (str): Custom analysis prompt
            days (int): Number of days to search back
            
        Returns:
            dict: Combined search and analysis results
        """
        started_at = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        
        print(f"üîç Searching Ahmia for: '{query}'")
        print(f"üìä Max sites to analyze: {max_sites}")
        print(f"üîç Crawl depth: {depth}")
        print(f"üìÖ Time filter: {days} days" if days else "üìÖ Time filter: Any time")
        
        # Step 1: Search Ahmia with fallback
        search_results = self.search_with_fallback(query, max_sites, days)
        
        if not search_results:
            return {
                "success": False,
                "error": "No search results found",
                "metadata": {
                    "started_at": started_at,
                    "query": query,
                    "max_sites": max_sites,
                    "depth": depth,
                    "days": days
                }
            }
        
        print(f"‚úÖ Found {len(search_results)} onion sites to analyze")
        
        # Step 2: Analyze each onion site
        analysis_results = []
        successful_analyses = 0
        
        for i, result in enumerate(search_results, 1):
            print(f"\nüìù Analyzing site {i}/{len(search_results)}: {result['name']}")
            print(f"   URL: {result['onion_url']}")
            
            if not result['onion_url']:
                print("   ‚ö†Ô∏è  No onion URL found, skipping...")
                continue
            
            # Analyze the onion site
            analysis = self.analyze_onion_site(
                result['onion_url'], 
                depth, 
                custom_prompt,
                model,
                use_langchain
            )
            
            # Combine search result with analysis
            combined_result = {
                **result,
                **analysis
            }
            
            analysis_results.append(combined_result)
            
            if analysis['success']:
                successful_analyses += 1
                print(f"   ‚úÖ Analysis completed successfully")
            else:
                print(f"   ‚ùå Analysis failed: {analysis.get('error', 'Unknown error')}")
            
            # Add delay between analyses to be respectful
            if i < len(search_results):
                time.sleep(2)
        
        # Step 3: Compile final results
        final_results = {
            "success": True,
            "query": query,
            "search_results_count": len(search_results),
            "successful_analyses": successful_analyses,
            "failed_analyses": len(search_results) - successful_analyses,
            "results": analysis_results,
            "metadata": {
                "started_at": started_at,
                "max_sites": max_sites,
                "depth": depth,
                "days": days,
                "total_processing_time": time.time() - time.mktime(time.strptime(started_at, "%Y-%m-%dT%H:%M:%SZ"))
            }
        }
        
        print(f"\nüéâ Bulk analysis completed!")
        print(f"   Total sites: {len(search_results)}")
        print(f"   Successful analyses: {successful_analyses}")
        print(f"   Failed analyses: {len(search_results) - successful_analyses}")
        
        return final_results
    
    def save_results(self, results: Dict, filename: Optional[str] = None) -> str:
        """Save results to JSON file"""
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            query_safe = re.sub(r'[^a-zA-Z0-9]', '_', results.get('query', 'search'))
            filename = f"ahmia_bulk_analysis_{query_safe}_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"üìÅ Results saved to: {filename}")
            return filename
            
        except Exception as e:
            print(f"‚ùå Error saving results: {e}")
            return ""
    
    def search_with_fallback(self, query: str, max_results: int = 10, days: Optional[int] = None) -> List[Dict]:
        """
        Search with fallback options if Ahmia fails
        """
        print(f"üîç Attempting Ahmia search for: '{query}' via internet")
        results = self.search_ahmia(query, max_results, days)
        
        if results:
            print(f"‚úÖ Ahmia search successful via internet, found {len(results)} results")
            return results
        
        print("‚ö†Ô∏è  Ahmia search failed, using fallback results...")
        
        # Final fallback: Create sample results for testing
        fallback_results = self._create_fallback_results(query, max_results)
        if fallback_results:
            print(f"üîÑ Using fallback results: {len(fallback_results)} sample sites")
            return fallback_results
        
        print("‚ùå All search methods failed")
        return []
    
        # Removed _search_ahmia_direct method - not needed since we use regular internet for Ahmia search
    
    def _create_fallback_results(self, query: str, max_results: int) -> List[Dict]:
        """
        Create fallback results for testing when search fails
        """
        print("üîÑ Creating fallback results for testing...")
        
        # Sample onion sites for testing (replace with real ones if needed)
        sample_sites = [
            {
                'name': f"Sample {query} site 1",
                'onion_url': f"http://sample1{query}.onion",
                'description': f"Sample {query} related site for testing purposes",
                'last_seen': "2025-08-14"
            },
            {
                'name': f"Sample {query} site 2", 
                'onion_url': f"http://sample2{query}.onion",
                'description': f"Another sample {query} site for testing",
                'last_seen': "2025-08-14"
            }
        ]
        
        return sample_sites[:min(max_results, len(sample_sites))]
